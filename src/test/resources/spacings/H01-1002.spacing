
Translating Legal Code
	9.0
	[-0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, -0.24029541, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0]

For training, we had available two parallel Chinese-English cor-
	9.0
	[-0.24030304, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397018, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, -0.24029541]

System
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0]

Baseline
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

pora distributed by the LDC: the complete Hong Kong legal code
	9.0
	[0.0, 0.0, 0.0, 2.6397018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24030304, 0.0, 0.0, 0.0, 2.6397018, 0.0, 2.6397018, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.6396942, -0.24029541, 0.0, 0.0, 2.400299, 0.0, -0.24029541, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0]

Training
	9.0
	[-0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Syntactic
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Semantic
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

ing: 24.58 megabytes, 2.67 million English words, 4.5 million Chi-
	9.0
	[0.0, 0.0, 0.0, 2.6397018, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, -0.24030304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0]

Translating Hong Kong News
	9.0
	[-0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 1.9197083, -0.24029541, 0.0, 0.0, 1.9197083, 0.0, -0.24029541, 0.0]

Training
	9.0
	[-0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

nary/phrasebook, which we also used.
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0]

LangModel
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

To determine the effects of varying amounts of training data on
	9.0
	[-0.7200012, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.6397018, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.6396942, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 2.400299, 0.0]

Syntactic
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

overall performance, we divided the bilingual training corpus into
	9.0
	[-0.24029922, -0.24029922, 0.0, 0.0, 0.0, 0.0, 2.8799973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.6396942, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0]

Semantic
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

ten nearly equal slices. Each test condition was then run ten times,
	9.0
	[0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0]

each time increasing the number of slices used for training the sys-
	9.0
	[0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0]

tem. After each training pass, the test sentences were translated and
	9.0
	[0.0, 0.0, 0.0, 2.8799973, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.199707, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0]

Figure 1: Judgements – Acceptable Translations
	9.0
	[0.0, 0.0, 0.0, 0.0, -0.24029541, 1.9197083, 0.0, 2.880005, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, -0.7200012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

the system’s performance evaluated automatically; selected points
	9.0
	[0.0, 0.0, 2.6397018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.47969818, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0]

were then manually evaluated for translation quality.
	9.0
	[0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4797058]

The automatic performance evaluation measured coverage of the
	9.0
	[0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 2.1600037, 0.0, 0.0]

used for Test Condition 5.
	9.0
	[0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 2.400299, -0.7200012, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0]

input and average phrase length. Coverage is the percentage of the
	9.0
	[0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 1.9197006, -0.24030304, -0.24030304, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.1600037, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 2.400299, 0.0, 0.0]

Figure 2 shows the proportion of the words in the test sentences
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 2.400299, 0.0, 0.0, -0.24029541, 0.0, 1.9197083, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.400299, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.400299, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.4003296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

input text for which a translation is produced by a particular trans-
	9.0
	[0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, -0.24030304, 0.0, 2.400299, 0.0, 0.0, 2.6397018, 0.0, 0.0, 0.0, 0.0, 2.1600037, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 2.1600037, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0]

for which the EBMT engine was able to produce a translation,
	9.0
	[0.0, 0.0, 4.079712, 0.0, 0.0, 0.0, 0.0, 3.600006, 0.0, 0.0, 3.600006, 0.0, 0.0, 0.0, 3.8403015, 0.0, 0.0, 0.0, 0.0, 0.0, 3.600006, 0.0, 0.0, 3.3597107, 0.0, 0.0, 0.0, 3.600006, 0.0, 3.8403015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.600006, 3.5999756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

while Figure 3 shows the average number of source-language words
	9.0
	[0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 1.6802979, 0.0, 0.0, -0.24029541, 0.0, 1.199707, 0.0, 0.0, 1.6802979, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96032715, 0.0, 0.0, 0.0, 0.0]

per translated fragment. These curves do not increase monotoni-
	9.0
	[0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.280304, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, -0.24029541, 0.0, 2.6397095, 0.0, 2.880005, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

length is a crude indication of translation quality – the longer the
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 2.6397018, 0.0, 2.8799973, 2.8799973, 0.0, 0.0, 0.0, 0.0, 2.6397018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 2.880005, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0]

cally because, for performance reasons, the EBMT engine does not
	9.0
	[0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9196777, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0]

phrase that is translated, the more context is incorporated and the
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.8799973, 0.0, 2.8799973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 2.6396942, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.6397095, 0.0, 0.0]

attempt to align every occurrence of a phrase, only the
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 1.6802979, -0.24029541, -0.24029541, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.199707, 0.0, 1.9197083, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0]

less likely it is that the wrong sense will be used in the translation or
	9.0
	[0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 1.9197006, 0.0, 1.9197006, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 1.199707, 0.0, 0.0, 0.0, 2.1600037, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.4400024, 0.0, 1.919693, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0]

corpus can cause EBMT to ignore matches that successfully align
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.6397095, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1599731, 0.0, 0.0, 0.0, 0.0]

nary and glossary remain constant for a given test condition, only
	9.0
	[0.0, 0.0, 0.0, 2.6397018, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 2.880005, 2.6396942, 0.0, -0.24029541, -0.24029541, 0.0, 2.6396942, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0]

in favor of newer occurrences which it is unable to align.
	9.0
	[0.0, 2.1600037, 0.0, -0.24029541, -0.24029541, 0.0, 2.400299, 0.0, 2.1600037, 0.0, -0.24029541, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 2.400299, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0]

the EBMT coverage will be presented.
	9.0
	[0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.1600037, 0.0, -0.24030304, -0.24030304, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 2.6396942, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Manual grading of the output was performed using a web-based
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.400299, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 1.919693, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

system with which the graders could assign one of three scores
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 3.359703, 0.0, 0.0, 0.0, 3.5999985, 0.0, 0.0, 0.0, 0.0, 3.5999985, 0.0, 0.0, 3.3596954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3596954, 0.0, 0.0, 0.0, 0.0, 3.3596954, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 3.3596954, 0.0, 3.600006, 0.0, 0.0, 0.0, 0.0, 3.600006, 0.0, 0.0, 0.0, 0.0, 0.0]

longer matches. In general, the closer training and test text are to
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.8403015, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.6397095, 0.0, -0.24029541, 0.0, 2.880005, 0.0, 0.0, 2.6397095, 0.0]

each other, the longer the phrases they have in common.
	9.0
	[0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, -0.24029541, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, -0.24029541, 2.1600037, 0.0, -0.24029541, -0.24029541, 2.1600037, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

correctness and meaning preservation. This type of quality scoring
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Figure 1 summarizes the results of human quality assessments.
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

is commonly used in assessing translation quality, and is used by
	9.0
	[0.0, 2.880001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397018, 0.0, 0.0, 0.0, 2.6397018, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4797058, 2.880005, 0.0, 0.0, 2.6396942, 0.0, 2.880005, 0.0, 0.0, 0.0, 2.6397095, 0.0]

The “Good” and “OK” judgements were combined into “Accept-
	9.0
	[0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 2.880005, -0.7199707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

other TIDES participants. Fifty-two test sentences were translated
	9.0
	[0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.600006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 2.4003143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

able” and the the percentage of “Acceptable” judgements was aver-
	9.0
	[0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 1.9197083, 0.0, 0.0, 1.9197083, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 2.1600037, -0.7200012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 1.9196777, -0.24029541, -0.24029541, 0.0, -0.24029541]

for each of four points from the automated evaluation and these sets
	9.0
	[0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 1.6802979, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0]

aged across sentences and graders. As hoped and expected, the im-
	9.0
	[0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 1.9197083, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 1.9196777, 0.0, 0.0]

of four alternatives presented to the graders. The four points chosen
	9.0
	[0.0, 1.9197006, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24030304, -0.24030304, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 1.919693, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0]

provements do in fact result not only in better coverage by EBMT,
	9.0
	[0.0, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.1600037, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 2.1599731, 0.0, 2.1599731, 0.0, 0.0, 0.0, -0.7199707]

were the baseline system with 100% of the training corpus, the full
	9.0
	[0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 1.919693, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0]

but also in better quality assessments by the human graders. Fur-
	9.0
	[-0.24029541, 0.0, 2.880005, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 2.6397095, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.3200073, 0.0, 0.0, -0.24029541]

system with 20% and 100% training, and the full system trained on
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 1.9197006, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 1.919693, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0]

ther, the results on Hong Kong news text show that the choice of
	9.0
	[0.0, 0.0, 0.0, -0.24029541, 3.1203003, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 2.6397095, 0.0, 0.0, 0.0, 2.880005, -0.24029541, 0.0, 0.0, 2.400299, 0.0, -0.24029541, 0.0, 2.6397095, 0.0, -0.24029541, 0.0, 3.1203003, 0.0, 0.0, -0.24029541, 2.6397095, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4003296, 0.0]

language model does have a definite effect on quality. These results
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.6802979, 0.0, -0.24029541, -0.24029541, 1.6802979, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, -0.24029541, 0.0, 0.0, 0.0, 1.9197083, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4797058, 2.6397095, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

were selected due to the difficulty and expense of obtaining large
	9.0
	[0.0, 0.0, 0.0, 2.8799973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.6397018, 0.0, 2.880005, 0.0, 0.0, 2.6396942, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 2.6396942, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, -0.24029541, 0.0]

also confirm the adage that there is no such thing as too much train-
	9.0
	[0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 1.9197083, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 1.6802979, 0.0, 0.0, 1.9196777, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0]

numbers of manual quality judgements.
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

ing text for language modeling, since the model generated from the
	9.0
	[0.0, 0.0, 1.9197083, 0.0, -0.24029541, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 2.1599731, 0.0, 0.0]

To assess the performance of the system in a different domain,
	9.0
	[-0.7200012, 2.6397018, 0.0, 0.0, 0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 2.8799973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 2.880005, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 2.880005, 2.6396942, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

EBMT corpus was unable to match the performance of the pre-
	9.0
	[0.0, 0.0, 0.0, 3.3597107, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 3.600006, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 3.3597107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3596802, 0.0, 3.3596802, 0.0, 0.0, 3.3596802, 0.0, 0.0, 0.0]

as well as the effect of the trigram language model on the selec-
	9.0
	[0.0, 2.8799973, 0.0, 0.0, 0.0, 3.1203003, 0.0, 2.8799973, 0.0, 0.0, 3.1203003, 0.0, -0.24030304, 0.0, 0.0, 0.0, 3.1202927, 0.0, 3.1203003, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 3.120285, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0]

existing model generated from two orders of magnitude more text.
	9.0
	[-0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9196777, 0.0, 0.0, 0.0, 2.1599731, 0.0, -0.24029541, 0.0, 0.0]

tion of translated fragments for the final translation, we obtained
	9.0
	[0.0, 0.0, 0.0, 3.1203003, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 3.1203003, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3596954, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

manual judgements for 44 sentences on an additional four test con-
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 2.1600037, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 1.919693, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0]

ditions, each trained with the entire available parallel text and tested
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, -0.24029541, 0.0, 1.919693, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 0.0, 0.0]

5. CONCLUSIONS AND FUTURE WORK
	12.0
	[0.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3599854, 0.0, 0.0, 0.0]

on Hong Kong news text rather than legal sentences. These points
	9.0
	[0.0, 2.400299, 0.0, 0.0, 0.0, 2.1600037, -0.24030304, 0.0, 0.0, 2.159996, 0.0, -0.24030304, 0.0, 2.1600037, 0.0, -0.24029541, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.400299, 0.0, -0.24029541, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0]

As seen in Figure 2, the enhancements described here cumula-
	9.0
	[0.0, 2.880005, 0.0, 0.0, 0.0, 2.400299, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 3.1203003, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

tively provide a 12% absolute improvement in coverage for EBMT
	9.0
	[0.0, -0.24029541, -0.24029541, 0.0, 0.0, 2.400299, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 1.9197083, 2.1600037, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.1600037, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 1.9196777, 0.0, 0.0, 2.4003296, 0.0, 0.0, 0.0]

different language models for within-domain training: an English
	9.0
	[0.0, 0.0, -0.24029922, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.8403015, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

translations without requiring any additional knowledge resources.
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, -0.24029541, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9196777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

language model derived from the legal corpus, one derived from
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397018, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0, 3.1203003, 0.0, 0.0, 0.0, 3.3596954, 0.0, 0.0, 3.1203003, 0.0, -0.24029541, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3596954, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0, 3.1203003, 0.0, 0.0, 0.0]

Further, the enhanced coverage does, in fact, result in improved
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 3.600006, 0.0, 0.0, 3.3597107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 3.3597107, 0.0, 3.3597107, 0.0, 0.0, 0.0, 0.0, 3.8403015, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3597107, 0.0, 3.3596802, 0.0, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0]

the news corpus, and a pre-existing model generated from two gi-
	9.0
	[0.0, 0.0, 2.6397018, 0.0, -0.24030304, 0.0, 2.159996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397018, 0.0, 0.0, 2.1600037, 2.6396942, 0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 2.6397095, 0.0, 0.0]

translations, as verified by human judgements. We can also con-
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 2.880005, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.5603027, -0.7200012, 3.1203003, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0]

gabytes of newswire and broadcast news transcriptions.
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 2.400299, 0.0, -0.24030304, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, -0.24029541, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

clude that when we combine words into larger chunks on both sides
	9.0
	[0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 1.4400024, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, -0.24029541, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.9196777, 0.0, 0.0, 0.0, 0.0]

of the corpus, the possibility of finding larger matches between the
	9.0
	[0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, -0.24029541, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9196777, 0.0, 0.0]

4. RESULTS
	12.0
	[0.0, 11.999996, 0.0, 0.0, 0.0, 0.0, -1.199997, 0.0]

source language and the target language increases, which leads to
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.6397095, 0.0, 0.0, -0.24029541, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 2.4003296, 0.0, 0.0, 0.0, 0.0, 2.4003296, 0.0]

the improvement of the translation quality for EBMT.
	9.0
	[0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, -0.7200012]

We discovered that there is a certain amount of synergy between
	9.0
	[-0.7200012, 1.9197006, 0.0, 0.0, 0.0, 0.0, -0.24030304, -0.24030304, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.1600037, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

We will do further research on the interaction between the im-
	9.0
	[-0.7200012, 3.1203003, 0.0, 0.0, 0.0, 3.1203003, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 2.6397095, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4003296, 0.0, 0.0, 2.880005, 0.0, 0.0]

some of the improvements, particularly the term finder and statis-
	9.0
	[0.0, 0.0, 0.0, 2.400299, 0.0, 2.8799973, 0.0, 0.0, 2.6397018, 0.0, 0.0, 0.0, 0.0, -0.24030304, -0.24030304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

proved segmenter, term finder and statistical dictionary builder, uti-
	9.0
	[0.0, 0.0, -0.24029541, -0.24029541, 0.0, 1.9197083, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 1.6802979, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9196777, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 1.9196777, 0.0, 0.0, 0.0]

tical dictionary extraction. Applying the term finder modifies the
	9.0
	[0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397018, -0.24030304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0399933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 2.880005, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0]

lizing the information provided by the statistical dictionary as feed-
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 1.9197083, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 1.9196777, 0.0, 0.0, 0.0, 0.0]

parallel corpus in such a way that it becomes more difficult for
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.5999985, 0.0, 0.0, 0.0, 0.0, 0.0, 3.359703, 0.0, 3.359703, 0.0, 0.0, 0.0, 3.3596954, 3.3596954, 0.0, 0.0, 3.3596954, 0.0, 0.0, 0.0, 3.600006, 0.0, 3.600006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 3.600006, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 3.600006, 0.0, 0.0]

back for the segmenter and term finder to modify their results. We
	9.0
	[0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.400299, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 2.4003296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, -0.7199707]

the EBMT engine to find matches which it can align, while adding
	9.0
	[0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 2.159996, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.400299, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0]

are also investigating the effects of splitting the EBMT training into
	9.0
	[0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.6802979, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 1.6802979, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 1.6803284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9196777, 0.0, 0.0, 0.0]

dictionary entries derived from the modified corpus eliminates that
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0]

multiple sets of topic-specific sentences, automatically separated
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.600006, 0.0, 0.0, 0.0, 3.1203003, 0.0, 3.3597107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1203003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

effect. As a result, we will not present the performance results for
	9.0
	[0.0, -0.24029922, 0.0, 0.0, 0.0, 0.0, 3.5999985, 0.0, 2.400299, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397018, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.880005, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0]

using clustering techniques.
	9.0
	[0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

The relatively low slope of the coverage curve also indicates that
	9.0
	[0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 2.1600037, 0.0, -0.24029541, 2.1600037, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 2.1600037, 0.0, 0.0, 1.9197083, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, -0.24029541, 1.9197083, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0]

the training corpus is sufficiently large. Our prior experience with
	9.0
	[0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.400299, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, -0.24029541, 0.0, 0.0, 3.8403015, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 2.6397095, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1599731, 0.0, 0.0, 0.0]

reflect the contribution of those two components to the full system
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24030304, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 2.400299, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0]

Improved Segmenter, term finder, statDict
	11.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1135864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1135864, 0.0, 0.0, 0.0, 3.1135864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1135864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Improved Segmenter, statDict
	11.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1135864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1135864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Improved Segmenter
	11.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1135864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Baseline system
	11.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1135864, 0.0, 0.0, 0.0, 0.0, 0.0]

Trained on News tested On Legalcode
	11.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1135864, 0.0, 3.1135864, 0.0, 0.0, 0.0, 3.1135864, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1135864, 0.0, 3.1135864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Figure 2: EBMT Coverage with Varying Training
	9.0
	[0.0, 0.0, 0.0, 0.0, -0.24029541, 2.1600037, 0.0, 2.880005, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 2.400299, -0.7200012, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, -0.7200012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

“Knowledge-Free” Example-Based Translation. In
	9.0
	[0.0, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0]

two and three million words of training text, which appears also to
	9.0
	[0.0, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, -0.24029541, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 1.9197083, 0.0]

Proceedings of the Seventh International Conference on
	9.0
	[0.0, -0.4797058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 1.4400024, 0.0]

Theoretical and Methodological Issues in Machine
	9.0
	[0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 2.400299, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0]

We have not yet taken full advantage of the features of the EBMT
	9.0
	[-0.7200012, 1.6802979, 0.0, -0.24030304, -0.24030304, 1.4400024, 0.0, 0.0, 1.9197006, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 1.199707, 0.0, 1.6802979, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 1.919693, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0]

July 1997.
	9.0
	[0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0]

software. In particular, it supports equivalence classes that permit
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0796967, 0.0, 2.6397018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 2.880005, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.6397095, 0.0, 0.0, 0.0, 0.0, 0.0]

http://www.cs.cmu.edu/˜ralf/papers.html
	9.0
	[0.6893921, 1.2923889, 0.7973938, 2.4623718, 2.7233887, 1.5713806, 0.9683838, 0.44369507, 0.20339966, 2.084381, 2.8197021, 1.3283691, 2.981659, 2.5794067, 0.8666687, 0.1133728, 2.2643738, 2.738678, 0.9144287, 0.50668335, 0.98638916, 1.661377, 1.6946716, 0.6893921, 1.4246521, 1.6524048, 1.6766968, 0.8963928, 0.8513794, 0.7046814, 0.8963623, 1.3616943, 1.0043945, 2.741394, 2.5046997, 0.6893921, 1.0197144, 0.7523804]

generalization of the training text into templates for improved cov-
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 2.159996, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, -0.24029541, 0.0, 2.400299, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0, 2.400299, 0.0, -0.24029541, 0.0]

erage. We intend to test automatic creation of equivalence classes
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 3.8403015, -0.7200012, 2.6397018, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.6396942, 0.0, 0.0, 0.0, 2.6396942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.6396942, 0.0, 0.0, 0.0, -0.24029541, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Example-Based Translation System. In Proceedings of the
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.4001465, 0.0, -0.4797058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 2.1599731, 0.0, 0.0]

ments reported herein.
	9.0
	[0.0, 0.0, 0.0, 0.0, 2.159996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Eighth International Conference on Theoretical and
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 1.4400024, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0]

pages 22–32, Chester, England, August 1999.
	9.0
	[0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0]

6. ACKNOWLEDGEMENTS
	12.0
	[0.0, 11.999996, -0.7200012, 0.0, 0.0, 0.0, -0.48000336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

http://www.cs.cmu.edu/˜ralf/papers.html
	9.0
	[0.6893921, 1.2923889, 0.7973938, 2.4623718, 2.7233887, 1.5713806, 0.9683838, 0.44369507, 0.20339966, 2.084381, 2.8197021, 1.3283691, 2.981659, 2.5794067, 0.8666687, 0.1133728, 2.2643738, 2.738678, 0.9144287, 0.50668335, 0.98638916, 1.661377, 1.6946716, 0.6893921, 1.4246521, 1.6524048, 1.6766968, 0.8963928, 0.8513794, 0.7046814, 0.8963623, 1.3616943, 1.0043945, 2.741394, 2.5046997, 0.6893921, 1.0197144, 0.7523804]

We would like to thank Alon Lavie and Lori Levin for their com-
	9.0
	[-0.7200012, 1.6802979, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 1.6802979, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 1.919693, 0.0, 0.0, 0.0, 1.6802979, 0.0, -0.24029541, 0.0, 0.0, 1.919693, 0.0, 0.0, 1.6802979, 0.0, 0.0, 0.0, 1.919693, 0.0, -0.24029541, 0.0, 0.0, 1.6802979, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0]

ments on drafts of this paper.
	9.0
	[0.0, 0.0, 0.0, 0.0, 2.159996, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 2.400299, 0.0, 2.159996, 0.0, 0.0, 0.0, 2.400299, 0.0, 0.0, 0.0, 0.0, -0.4797058]

Examples. In Proceedings of the Eighteenth International
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 2.400238, 0.0, -0.4797058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 2.400299, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

7. REFERENCES
	12.0
	[0.0, 11.999996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

ANGLOSS System. In Proceedings of the Sixteenth
	8.658537
	[0.48023987, 0.48023987, 0.48023987, 0.48023987, 0.48023987, 0.48023987, 1.6785583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 2.1600037, 0.0, -0.4797058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6802979, 0.0, 2.1600037, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Linguistic Data Consortium. In Proceedings of the 1994 ARPA
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197083, 0.0, 2.400177, 0.0, -0.4797058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 2.1600037, 0.0, 0.0, 2.1599731, 0.0, 0.0, 0.0, 1.6802979, 0.0, 0.0, -0.7199707]

International Conference on Computational Linguistics, pages
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24029541, 0.0, 0.0, 0.0, 1.4400024, 0.0, 1.919693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.544922E-4, 1.9197083, 0.0, 0.0, 0.0, 0.0]

Human Language Technology Workshop. Morgan Kaufmann,
	9.0
	[0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, -0.7200012, 0.0, -0.24029541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, -0.7200012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -3.0517578E-5, 1.6802979, 0.0, 0.0, -0.24029541, 0.0, 0.0, 1.9197083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

169–174, Copenhagen, Denmark, 1996.
	9.0
	[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9197006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4400024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1600037, 0.0, 0.0, 0.0, 0.0]

http://www.cs.cmu.edu/˜ralf/papers.html
	9.0
	[0.68940735, 1.2924042, 0.7974014, 2.46241, 2.723404, 1.5714035, 0.96839905, 0.44371033, 0.20339966, 2.324707, 2.5794067, 1.3283844, 2.9816742, 2.5793762, 0.86668396, 0.11338806, 2.2644043, 2.738678, 0.91438293, 0.50668335, 0.9864044, 1.9016724, 1.4543915, 0.6893921, 1.4246826, 1.6523895, 1.6766815, 0.89637756, 0.85139465, 0.7046814, 0.8963928, 1.3616791, 1.0043945, 2.9816742, 2.264389, 0.6893921, 1.0196838, 0.75234985]
